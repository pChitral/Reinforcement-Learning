{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = LawnmowerEnv(max_steps=100, alpha=0.5, gamma=0.9, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performace :  0.07\n",
      "[[36.09263464 30.35561939 32.45963105 36.86205384]\n",
      " [33.02654009 27.92641182 27.00887321 35.82426229]\n",
      " [26.21121197 19.8820103  20.53375167 30.97327178]\n",
      " [18.31272957 17.97156786 18.29592159 25.81989225]\n",
      " [34.8730991  21.80380099 27.68482021 27.07566991]\n",
      " [31.3243344  23.63778102 21.08820936 25.96711932]\n",
      " [25.08418672 18.55558595 16.89828621 21.8007756 ]\n",
      " [18.89740066 13.36075065 15.10139646 16.60189167]\n",
      " [26.16183417 16.26384942 22.09887539 19.97170276]\n",
      " [26.10928823 16.08337423 19.90755169 20.45506704]\n",
      " [19.56947002 16.00493069 12.16378913 19.91447923]\n",
      " [12.40963564  5.10065848  9.81119747 15.42833809]\n",
      " [17.67715771 13.52134015 15.5474692  11.57340761]\n",
      " [20.38308955 12.63490841 16.39001862 13.37540791]\n",
      " [17.91264774 13.98042037  5.40335846 14.00560344]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LawnmowerEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, max_steps = 100):\n",
    "        # Define the observation and action spaces\n",
    "        self.observation_space = spaces.Discrete(16)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Initialize the grid\n",
    "        self.grid = np.array([[6, 5, 6, 5],\n",
    "                              [5, 6,  5, 6],\n",
    "                              [5,  6, 6,  6],\n",
    "                              [6, 5,  6, 6]])\n",
    "\n",
    "        # Initialize the agent and goal positions\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.goal_pos = np.array([3, 3])\n",
    "\n",
    "        # Initialize the reward\n",
    "        self.reward = 0\n",
    "        # self.history = []\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        self.current_step = 0\n",
    "        if np.array_equal(self.agent_pos, self.goal_pos):\n",
    "            self.agent_pos = np.array([0, 0])\n",
    "\n",
    "        # Reset the agent position and reward\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.reward = 0\n",
    "\n",
    "        # Return the initial observation\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "        # Move the agent based on the chosen action\n",
    "        if action == 0:\n",
    "            self.agent_pos[0] -= 1  # Up\n",
    "        elif action == 1:\n",
    "            self.agent_pos[0] += 1  # Down\n",
    "        elif action == 2:\n",
    "            self.agent_pos[1] += 1  # Right\n",
    "        elif action == 3:\n",
    "            self.agent_pos[1] -= 1  # Left\n",
    "\n",
    "        # Keep the agent within the grid boundaries\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 3)\n",
    "\n",
    "        # Calculate the reward based on the new agent position\n",
    "        self.reward = self.grid[tuple(self.agent_pos)]\n",
    "        # self.history.append({\"observation\": self._get_observation(), \"reward\": self.reward, \"done\": done})\n",
    "        # Check if the episode is done\n",
    "        # self.reward > 0\n",
    "        done = bool(  np.array_equal(\n",
    "            self.agent_pos, self.goal_pos))\n",
    "        \n",
    "        # self.history.append({\"observation\": self._get_observation(), \"reward\": self.reward, \"done\": done})\n",
    "        # Return the new observation, reward, done flag, and info dictionary\n",
    "        return self._get_observation(), self.reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "     \n",
    "        # Plot the grid with markers for different tile types\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if self.grid[i][j] < 0:\n",
    "                    ax.scatter(j, i, marker='X', color='r', s=1000)\n",
    "                elif self.grid[i][j] == 0:\n",
    "                    ax.scatter(j, i, marker='o', color='b', s=1000)\n",
    "                elif self.grid[i][j] > 0:\n",
    "                    ax.scatter(j, i, marker='o', color='g', s=1000)\n",
    "\n",
    "        # Plot the agent and goal positions\n",
    "        ax.scatter(self.agent_pos[1], 3 - self.agent_pos[0],\n",
    "                   marker='s', color='y', s=1000)\n",
    "        ax.scatter(self.goal_pos[1], 3 - self.goal_pos[0],\n",
    "                   marker='s', color='m', s=1000)\n",
    "\n",
    "        # Set the x and y axis limits\n",
    "        ax.set_xlim([-0.5, 3.5])\n",
    "        ax.set_ylim([-0.5, 3.5])\n",
    "\n",
    "        # Add title and axis labels\n",
    "        ax.set_title('Lawnmower Environment')\n",
    "        ax.set_xlabel('X Position')\n",
    "        ax.set_ylabel('Y Position')\n",
    "\n",
    "        # Add gridlines\n",
    "        ax.grid(which='major', color='gray', linestyle='-', linewidth=2)\n",
    "        ax.set_xticks(np.arange(-0.5, 4, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, 4, 1))\n",
    "        ax.xaxis.tick_top()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Convert the agent position to an observation index\n",
    "        return np.ravel_multi_index(tuple(self.agent_pos), (4, 4))\n",
    "\n",
    "\n",
    "\n",
    "env = LawnmowerEnv()\n",
    "# Parameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 100\n",
    "max_steps = 100\n",
    "alpha = 0.05\n",
    "gamma = 0.95\n",
    "  \n",
    "#Initializing the Q-vaue\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Function to choose the next action with episolon greedy\n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "    \n",
    "#Initializing the reward\n",
    "reward=0\n",
    "  \n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "    t = 0\n",
    "    state1 = env.reset()\n",
    "    action1 = choose_action(state1)\n",
    "  \n",
    "    while t < max_steps:\n",
    "        # Visualizing the training\n",
    "        # env.render()\n",
    "          \n",
    "        # Getting the next state\n",
    "        state2, reward, done, info = env.step(action1)\n",
    "  \n",
    "        #Choosing the next action\n",
    "        action2 = choose_action(state2)\n",
    "          \n",
    "        #Learning the Q-value\n",
    "        Q[state1, action1] = Q[state1, action1] + alpha * (reward + gamma * Q[state2, action2] - Q[state1, action1])\n",
    "  \n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "          \n",
    "        #Updating the respective vaLues\n",
    "        t += 1\n",
    "        reward += 1\n",
    "          \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "#Evaluating the performance\n",
    "print (\"Performace : \", reward/total_episodes)\n",
    "  \n",
    "#Visualizing the Q-matrix\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
